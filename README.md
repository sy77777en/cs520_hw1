# LLM Code Generation Assignment

**Author:** Siyuan Cen  
**Email:** siyuanc4@andrew.cmu.edu  
**Date:** October 20, 2025  

---

## ğŸ§© Overview

This repository contains all code, prompts, and evaluation results for the **LLM Code Generation Assignment**. 
The project compares **GPT-4o (OpenAI)** and **Claude Sonnet 4.5 (Anthropic)** on Python code generation tasks, investigates failure cases, and introduces an improved prompting method called **Signature-Aware Prompting (SAP)**.

---

## âš™ï¸ Experimental Setup

### Models Tested
| Model             | Provider  | Access Method                                                |
| ----------------- | --------- | ------------------------------------------------------------ |
| GPT-4o            | OpenAI    | API (via `eval_gpt.py`)                                      |
| Claude Sonnet 4.5 | Anthropic | WebUI (manual prompt & code evaluation using `test_claude_webui_output.py`) |

### Datasets
- **HumanEval** (4 problems)  
- **MBPP** (2 problems)  
- **APPS** (2 problems)  
- **SWE-bench** (2 problems)  

**Total:** 10 problems evaluated per model.

### Prompting Strategies
1. **Baseline:** â€œWrite ONLY the Python code without explanations.â€  
2. **Chain-of-Thought:** â€œThink step-by-step: (1) Understand (2) Edge cases (3) Plan (4) Code.â€  
3. **Self-Planning:** â€œCreate a plan first, then implement.â€  
4. **Self-Debugging:** â€œWrite code, then mentally test with examples.â€

---

## ğŸ§  Repository Structure

```
LLM-CodeGen-Assignment/
â”œâ”€â”€ prompts/
â”‚ â”œâ”€â”€ baseline.txt
â”‚ â”œâ”€â”€ cot.txt
â”‚ â”œâ”€â”€ self_planning.txt
â”‚ â”œâ”€â”€ self_debugging.txt
â”‚ â”œâ”€â”€ MBPP_25_refined.txt
â”‚ â””â”€â”€ MBPP_519_refined.txt
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ eval_gpt.py # Evaluate GPT models via OpenAI API
â”‚ â””â”€â”€ test_claude_webui_output.py # Evaluate Claude outputs copied from WebUI
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ gpt4o_result.json
â”‚ â”œâ”€â”€ claude4.5_result.json
â”‚ â”œâ”€â”€ refined_prompt_result.json
â”œâ”€â”€ report/
â”‚ â””â”€â”€ Siyuan_Cen_LLM_CodeGen_Report.pdf
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Evaluate GPT Models via OpenAI API

**File:** `scripts/eval_gpt.py`

This script automatically sends prompts to GPT models via the OpenAI API, executes the returned code, and reports pass/fail results.

```bash
export OPENAI_API_KEY="your_api_key_here"
python scripts/eval_gpt.py
```

**Default model:** `gpt-4o`
 You can modify it to `"gpt-5"` or another model name inside the script.
 All results are saved under `results/results.json`.

------

### 2ï¸âƒ£ Evaluate Claude Models via WebUI (Manual Process)

**File:** `scripts/test_claude_webui_output.py`

Claude Sonnet 4.5 does not have API access in this setup, so evaluation is done **manually** through Anthropicâ€™s Web Interface.

#### Steps:

1. **Copy the correct prompt** (e.g., from `/prompts/MBPP_25_refined.txt`)
    Paste it directly into Claudeâ€™s WebUI input box.

2. **Let Claude generate the Python function.**
    Wait until it outputs the code (usually wrapped in triple backticks).

3. **Copy the output function** and save it into a local text file (e.g., `claude_MBPP_25.txt`).

4. **Run the testing script:**

   ```
   python scripts/test_claude_webui_output.py problems/MBPP_25.json claude_MBPP_25.txt
   ```

5. The script will:

   - Clean up any markdown fences (`python â€¦ `).
   - Execute the code safely in Python.
   - Run all dataset test cases.
   - Report which ones pass or fail.

#### Example Output

```
ğŸ“ Task ID: MBPP_25
ğŸ¯ Entry Point: find_Product
âœ… PASSED 4/4 tests
```

This procedure was repeated for all 10 problems across the datasets.

------

## ğŸ“Š Evaluation Method

Each modelâ€™s generated code is executed against ground-truth test assertions defined in each dataset JSON.

- âœ… **PASS:** all assertions succeeded
- âŒ **FAIL:** at least one assertion failed

Overall **Pass@1** accuracy is calculated as:

\text{Pass@1} = \frac{\text{# Problems Passed}}{\text{# Total Problems}}

------

## ğŸ§© Innovation: Signature-Aware Prompting (SAP)

To fix datasetâ€“prompt inconsistencies (especially in MBPP), the **Signature-Aware Prompting** method automatically injects explicit function signatures into prompts.

Example prompt modification:

```
You must use the function signature:
def find_Product(arr, n):
```

This refinement ensures the model outputs code compatible with the datasetâ€™s expected interface.